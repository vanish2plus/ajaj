Измерение информации это важный аспект в изучении теории информации, которая занимается анализом, передачей, хранением и управлением информацией. Существует несколько подходов к измерению информации, каждый из которых имеет свои особенности и области применения. 

Вот некоторые из них: 
* Шенноновская энтрапия
* Колмогоровская  сложность
* Мутуальная информация
* Кросс-энтропия
* Дженсон - Шэннон
* Энтропия Цаллиса

Эти подходы используются в различных областях, включая телекоммуникации, машинное обучение, статистику, криптографию и биоинформатику.

Шенноновская энтропия - это один из самых известных подходов, предложенный Клодом Шенноном. Энтропия измеряет среднее количество информации, необхоимое для определения состояния источника информации. Вычисляется на основе вероятностей различны состояний источника и выражается в битах.

Пример:

Имеется игральная кость с шесьтю гранями., на каждой из его сторон число от 1 до 6. Вероятность выпадения каждого числа равна 1/6. Метод Шенноновской энтропии позволяет измерить, насколько непредсказуемым яляется бросок кубика.

H = −n∑i=1PilogPi - формула

Шаги расчета

Определите вероятности каждого события
Пусть у нас есть множество событий X = {x1, x2, ..., xn} с соответствующими вероятностями p(x1), p(x2), ..., p(xn).

Вычислите логарифм каждой вероятности по основанию 2:

log2(p(x1)), log2(p(x2)), ..., log2(p(xn))


Умножьте каждую вероятность на ее логарифм:

p(x1) * log2(p(x1)), p(x2) * log2(p(x2)), ..., p(xn) * log2(p(xn))

Суммируйте полученные значения:

∑ (p(x) * log2(p(x))) = p(x1) * log2(p(x1)) + p(x2) * log2(p(x2)) + ... + p(xn) * log2(p(xn))

Умножьте полученную сумму на -1:
